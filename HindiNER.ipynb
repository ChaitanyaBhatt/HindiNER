{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
    "from keras.models import Model, Input\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "data=pd.read_excel('Hindi NER.xlsx', header=None)\n",
    "Data=pd.DataFrame(data)\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing the sentences.\n",
    "count=1\n",
    "for i in range(0,1610):\n",
    "    if pd.isnull(Data.loc[i][0]):\n",
    "        Data.drop(Data.index[i])\n",
    "        count=count+1\n",
    "    else:\n",
    "        Data.at[i,2]= count\n",
    "Data.columns = ['Word', 'Tag', 'Sentence #'] \n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class to get sentence. The each sentence will be list of tuples with its tag.\n",
    "class sentence(object):\n",
    "    def __init__(self, Data):\n",
    "        self.n_sent = 1\n",
    "        self.df = Data\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, t) for w, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying one full sentence\n",
    "getter = sentence(Data)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "# Number of data points passed in each iteration\n",
    "batch_size = 64 \n",
    "# Passes through entire dataset\n",
    "epochs = 8\n",
    "# Maximum length of review\n",
    "max_len = 75 \n",
    "# Dimension of embedding vector\n",
    "embedding = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting unique words and labels from data\n",
    "words = list(Data['Word'].unique())\n",
    "tags = list(Data['Tag'].unique())\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "# label is key and value is index.\n",
    "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i: w for w, i in word_to_index.items()}\n",
    "idx2tag = {i: w for w, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The word अरे is identified by the index: {}\".format(word_to_index[\"अरे\"]))\n",
    "print(\"The label U-location is identified by the index: {}\".format(tag_to_index[\"U-location\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each sentence into list of index from list of tokens\n",
    "X = [[word_to_index[w[0]] for w in s] for s in sentences]\n",
    "\n",
    "# Padding each sequence to have same length  of each word\n",
    "X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label to index\n",
    "y = [[tag_to_index[w[2]] for w in s] for s in sentences]\n",
    "\n",
    "# padding\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = \"post\", value = tag_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tag = Data['Tag'].nunique()\n",
    "# One hot encoded labels\n",
    "y = [to_categorical(i, num_classes = num_tag + 1) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training input data : \", X_train.shape)\n",
    "print(\"Size of training output data : \", np.array(y_train).shape)\n",
    "print(\"Size of testing input data : \", X_test.shape)\n",
    "print(\"Size of testing output data : \", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first sentence before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First label before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Data['Tag'].nunique()\n",
    "# Model architecture\n",
    "input = Input(shape = (max_len,))\n",
    "model = Embedding(input_dim = len(words) + 2, output_dim = embedding, input_length = max_len, mask_zero = True)(input)\n",
    "model = Bidirectional(LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
    "crf = CRF(num_tags+1)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To checkpoint best performance after each epoch.\n",
    "checkpointer = ModelCheckpoint(filepath = 'model.h5',\n",
    "                       verbose = 0,\n",
    "                       mode = 'auto',\n",
    "                       save_best_only = True,\n",
    "                       monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, np.array(y_train), batch_size=batch_size, epochs=epochs,\n",
    "                    validation_split=0.1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
